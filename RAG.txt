ðŸ”¹ Basic RAG Interview Questions
1. What is Retrieval-Augmented Generation (RAG)?
Answer:
RAG is an AI technique that combines retrieval (fetching relevant information from a knowledge base/vector database) with generation (using an LLM to produce responses). Instead of relying only on the LLMâ€™s training data, it retrieves the most relevant documents and augments the prompt to generate more accurate, contextual, and up-to-date answers.

2. Why do we use RAG?
Answer:
	â€¢ Reduces hallucination in LLMs
	â€¢ Provides up-to-date domain knowledge
	â€¢ Handles proprietary/private data
	â€¢ Improves accuracy in enterprise search, chatbots, and Q&A systems

3. What are the main components of a RAG system?
Answer:
	1. Retriever â€“ Finds relevant documents (e.g., FAISS, Pinecone, Azure Cognitive Search).
	2. Generator â€“ LLM that uses retrieved context to produce responses (e.g., GPT, LLaMA).
	3. Knowledge Base â€“ Where data is stored, often in vector embeddings.
	4. Orchestrator / Pipeline â€“ Combines retrieval + generation.

4. What is the difference between RAG and fine-tuning?
Answer:
	â€¢ Fine-tuning: Updates model weights with new training data â†’ costly and static.
	â€¢ RAG: Keeps model fixed, retrieves external knowledge dynamically â†’ flexible and cheaper.

ðŸ”¹ Intermediate RAG Interview Questions
5. How are documents stored and retrieved in RAG?
Answer:
	â€¢ Documents are converted into embeddings (dense vector representations) using embedding models.
	â€¢ These vectors are stored in a vector database.
	â€¢ At query time, the user query is also embedded, and similarity search (e.g., cosine similarity, dot product) retrieves the most relevant documents.

6. What are some popular vector databases used in RAG?
Answer:
	â€¢ FAISS (Facebook AI Similarity Search)
	â€¢ Pinecone
	â€¢ Weaviate
	â€¢ Milvus
	â€¢ Azure Cognitive Search with Vector Indexing

7. What are common challenges with RAG?
Answer:
	â€¢ Choosing the right chunk size for documents
	â€¢ Balancing recall vs. precision in retrieval
	â€¢ Latency (retrieval + generation can be slow)
	â€¢ Keeping the knowledge base updated
	â€¢ Handling multi-turn conversations with memory

8. How does chunking affect RAG performance?
Answer:
	â€¢ Too small chunks â†’ retrieval may miss context.
	â€¢ Too large chunks â†’ irrelevant content included, higher token cost.
	â€¢ Optimal chunk size depends on domain, usually 200â€“500 words per chunk.

ðŸ”¹ Advanced RAG Interview Questions
9. How do you evaluate a RAG system?
Answer:
	â€¢ Quantitative metrics:
		â—‹ Retrieval precision/recall
		â—‹ MRR (Mean Reciprocal Rank)
		â—‹ BLEU/ROUGE for generation quality
	â€¢ Qualitative metrics:
		â—‹ Human evaluation for factual accuracy
		â—‹ User satisfaction scores
	â€¢ Groundedness checks: Ensuring answers are based on retrieved context

10. What optimizations can improve RAG performance?
Answer:
	â€¢ Use Hybrid Search (vector + keyword)
	â€¢ Apply Re-ranking models (e.g., BERT re-ranker) after initial retrieval
	â€¢ Cache embeddings and responses for frequent queries
	â€¢ Use query expansion or embedding fine-tuning for better semantic retrieval
	â€¢ Apply memory mechanism for multi-turn chat

11. What are some use cases of RAG in the real world?
Answer:
	â€¢ Enterprise chatbots (customer support with private data)
	â€¢ Legal & compliance Q&A
	â€¢ Healthcare assistants with medical knowledge bases
	â€¢ E-commerce product search
	â€¢ Code assistants retrieving documentation

12. How does RAG help reduce hallucinations?
Answer:
Instead of relying only on the LLMâ€™s parametric memory, RAG grounds answers in retrieved documents. Since responses are based on external verified sources, hallucinations (confident but false answers) are reduced.
